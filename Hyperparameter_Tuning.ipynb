{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformer_hyperparam import get_compiled_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "class ASLDataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, dataset_path: str, max_phrase_length: int, max_sign_length: int, batch_size: int, train: bool = True):\n",
    "        super().__init__()\n",
    "        self.ds_path = dataset_path\n",
    "        self.files = os.listdir(dataset_path)\n",
    "        self.max_phrase_length = max_phrase_length\n",
    "        self.max_sign_length = max_sign_length\n",
    "        self.unwanted_columns = ['sequence_id', 'frame', 'participant_id', 'phrase']\n",
    "        self.train = train\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)//self.batch_size\n",
    "\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.files)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.train:\n",
    "            self.shuffle()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # TODO: integrate batch_size --> take all examples from idx to idx + batch_size\n",
    "        batch_signs = []\n",
    "        batch_contexts = []\n",
    "        batch_phrases = []\n",
    "        for i in range(idx, idx+self.batch_size):\n",
    "            df_phrase = pd.read_pickle(self.ds_path + '/' + self.files[idx])\n",
    "            phrase = df_phrase.iloc[0].phrase\n",
    "            if len(phrase) > self.max_phrase_length:\n",
    "                print('Houston we have a problem!')\n",
    "\n",
    "            while len(phrase) < self.max_phrase_length:\n",
    "                phrase.append(59)\n",
    "            context = deepcopy(phrase)\n",
    "            context.insert(0, 60)\n",
    "            context = context[:-1]\n",
    "\n",
    "            df_phrase = df_phrase.drop(self.unwanted_columns, axis=1)\n",
    "            signs = df_phrase.to_numpy(copy=True)\n",
    "\n",
    "            if signs.shape[0] > self.max_sign_length:\n",
    "                print('Hooooouuuuusteeeeeen!')\n",
    "            signs = np.pad(signs, [(0, self.max_sign_length-signs.shape[0]), (0,0)], 'edge')\n",
    "            batch_signs.append(signs)\n",
    "            batch_contexts.append(context)\n",
    "            batch_phrases.append(phrase)\n",
    "\n",
    "        batch_signs = np.array(batch_signs)\n",
    "        batch_contexts = np.array(batch_contexts)\n",
    "        batch_phrases = np.array(batch_phrases)\n",
    "\n",
    "        return [batch_signs, batch_contexts], batch_phrases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_PHRASE_LENGTH = 100\n",
    "MAX_SIGN_LENGTH = 900\n",
    "BATCH_SIZE=32\n",
    "\n",
    "data_gen_train = ASLDataGenerator('./dataset complete/preprocessed_files_data_generator/train_ones', MAX_PHRASE_LENGTH, MAX_SIGN_LENGTH, batch_size=BATCH_SIZE)\n",
    "data_gen_train.shuffle()\n",
    "\n",
    "\n",
    "data_gen_test = ASLDataGenerator('./dataset complete/preprocessed_files_data_generator/test_ones', MAX_PHRASE_LENGTH, MAX_SIGN_LENGTH, train=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "data_gen_val = ASLDataGenerator('./dataset complete/preprocessed_files_data_generator/val_ones', MAX_PHRASE_LENGTH, MAX_SIGN_LENGTH, train=False, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import optuna\n",
    "import json\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "def objective(trial):\n",
    "    d_model = trial.suggest_int('d_model', 20, 256)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 10)\n",
    "    num_heads=trial.suggest_int('num_heads', 2, 10)\n",
    "    ff_dim=trial.suggest_int('ff_dim', 32, 1024)\n",
    "    dropout_rate=trial.suggest_float('droupout_rate', 0., 0.6)\n",
    "\n",
    "    with open (\"./dataset complete/character_to_prediction_index.json\", \"r\") as f:\n",
    "        characters = json.load(f)\n",
    "\n",
    "    output_vocab_size = len(characters) + 2\n",
    "\n",
    "    get_compiled_transformer(d_model, num_layers, num_heads, ff_dim, dropout_rate, output_vocab_size)\n",
    "\n",
    "    callbacks = [\n",
    "        optuna.integration.TFKerasPruningsCallback(trial, 'val_loss'),\n",
    "\n",
    "    ]\n",
    "    history = transformer.fit(data_gen_train, epochs=epochs, batch_size=BATCH_SIZE, validation_data=data_gen_val, callbacks=callbacks)\n",
    "\n",
    "    eval_results = transformer.evaluate(data_gen_test, batch_size = BATCH_SIZE)\n",
    "    print(f'Result:{float(eval_results[0]) }')\n",
    "\n",
    "    return float(eval_results[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.SuccessiveHalvingPruner(), sampler=optuna.samplers.TPESampler)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(states=[optuna.trial.TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(states=[optuna.trial.TrialState.COMPLETE])\n",
    "\n",
    "print('Study statistics:')\n",
    "print('   Number of finished Trials: ', len(study.trials))\n",
    "print('   Number of pruned Trials: ', len(pruned_trials))\n",
    "print('   Number of complete Trials: ', len(complete_trials))\n",
    "\n",
    "print('Best Trial: ')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  MSE: ', trial.value)\n",
    "\n",
    "print('   Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('   {}: {}'.format(key, value))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
