{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Kaggle Competition: https://www.kaggle.com/competitions/asl-fingerspelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploring Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparquet\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpq\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T19:47:33.614120200Z",
     "start_time": "2023-06-20T19:47:32.860408500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Looking at information within train.csv ans supplemental_metadata.csv\n",
    "df_train = pd.read_csv('./dataset complete/train.csv')\n",
    "df_supp_mat = pd.read_csv('./dataset complete/supplemental_metadata.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_supp_mat.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df_supp_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From [here](https://www.kaggle.com/competitions/asl-fingerspelling/discussion/416507https://www.kaggle.com/competitions/asl-fingerspelling/discussion/416507) supplementary material is unnecessary. For now, do not use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read in first parquet file\n",
    "first_table = pq.read_table('./dataset complete/train_landmarks/105143404.parquet')\n",
    "first_table = first_table.to_pandas()\n",
    "first_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We see for each sequence_id, the frames start again from 0\n",
    "# man kann auch schlussfolgern in dieser datei sind 1000 sätze als vektoren dargestellt\n",
    "zeros_table = first_table[first_table['frame'] == 0]\n",
    "zeros_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We see that the sequence_ids  are not in consecutive order the next sequence id after 1784552841 ist 1784574169\n",
    "first_table[first_table.index == 1784552842]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sequences are not sequentially numbered.\n",
    "\n",
    "One sequence corresponds to one sentence.\n",
    "\n",
    "Frame numberings restart at 0 for each sequence.\n",
    "\n",
    "One sequence might contain n frames."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We print all existinmg columns\n",
    "for column in first_table.columns.tolist():\n",
    "    print(column)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://camo.githubusercontent.com/7fbec98ddbc1dc4186852d1c29487efd7b1eb820c8b6ef34e113fcde40746be2/68747470733a2f2f6d65646961706970652e6465762f696d616765732f6d6f62696c652f706f73655f747261636b696e675f66756c6c5f626f64795f6c616e646d61726b732e706e67\" width=\"500\">\n",
    "<br>\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSHiCz-z3dQ93Zh4p4zq-0dk4A535Dqp5OTHpnqoklaQQ&s\" width=\"500\">\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/google/mediapipe/a908d668c730da128dfa8d9f6bd25d519d006692/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\" width=\"500\">\n",
    "\n",
    "It might make sense to use only hands, mouth/lips, eyes, and eyebrows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using isna() to select all rows with NaN under an entire DataFrame:\n",
    "df_nan = first_table[first_table.isna().any(axis=1)]\n",
    "df_nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove all pose columns except for the arm and wrist landmarks\n",
    "to_remove = []\n",
    "for column in first_table.columns.tolist():\n",
    "    if 'pose' in column:\n",
    "        if '11' in column:\n",
    "            continue\n",
    "        elif '12' in column:\n",
    "            continue\n",
    "        elif '13' in column:\n",
    "            continue\n",
    "        elif '14' in column:\n",
    "            continue\n",
    "        elif '15' in column:\n",
    "            continue\n",
    "        elif '16' in column:\n",
    "            continue\n",
    "        else:\n",
    "            to_remove.append(column)\n",
    "\n",
    "first_table = first_table.drop(to_remove, axis=1)\n",
    "first_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_nan = first_table[first_table.isna().any(axis=1)]\n",
    "df_nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for column in first_table.columns.tolist():\n",
    "    if 'face' in column:\n",
    "        to_remove.append(column)\n",
    "\n",
    "first_table = first_table.drop(to_remove, axis=1)\n",
    "\n",
    "df_nan = first_table[first_table.isna().any(axis=1)]\n",
    "df_nan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_nan.index.unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test = df_nan[df_nan.index == 1816603189]\n",
    "df_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for column in first_table.columns.tolist():\n",
    "    print(column)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have to much nan values even when leaving out pose. Consequently, we need to find a good interpolation scheme."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "files = os.listdir('./dataset complete/train_landmarks')\n",
    "len(files)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# interpolation in cas it doesn't work how we want it to work\n",
    "'''train_landmarks = remove_unwanted_columns(train_landmarks)\n",
    "# merge the hand and arm data with the corresponding participant id and phrase\n",
    "df = merge_datasets(train_landmarks, df_train)\n",
    "\n",
    "\n",
    "phrases = df.sequence_id.unique()\n",
    "removed_ids = []\n",
    "df_concat = pd.DataFrame()\n",
    "INTERPOLATION_NAN_GAP = 30\n",
    "FILL_NAN_VALUE = -1\n",
    "SKIP_VALUE = 10000000\n",
    "pd.options.mode.chained_assignment = None\n",
    "#for i in 0:\n",
    "phrase_table = df[df['sequence_id'] == 1784574169]\n",
    "#nan_count_left_hand = phrase_table['x_left_hand_0'].isna().sum()\n",
    "#nan_count_right_hand = phrase_table['x_right_hand_0'].isna().sum()\n",
    "#length_table = len(phrase_table)\n",
    "#if nan_count_left_hand > 0.75*length_table and nan_count_right_hand > 0.75*length_table:\n",
    "#    continue\n",
    "for column in phrase_table.columns.tolist():\n",
    "    #for column in ['x_left_hand_0', 'x_left_hand_1']:\n",
    "    if(column == 'sequence_id') or (column == 'frame') or (column == 'participant_id') or (column == 'phrase'):\n",
    "        continue\n",
    "    nan_count = phrase_table[column].isnull().astype(int).groupby(phrase_table[column].notnull().cumsum()).cumsum()\n",
    "    tmp = 0\n",
    "    for x in range(phrase_table.index[0],phrase_table.index[0] + len(nan_count)-1):\n",
    "        curr = nan_count[x]\n",
    "        next = nan_count[x+1]\n",
    "        if next > curr:\n",
    "            tmp = next\n",
    "            if (x == 541):\n",
    "                if tmp > INTERPOLATION_NAN_GAP:\n",
    "                    for j in range((x+2)-tmp, x+2):\n",
    "                        nan_count[j] = SKIP_VALUE\n",
    "                tmp = 0\n",
    "        elif next == curr:\n",
    "            tmp = 0\n",
    "            continue\n",
    "        elif (next < curr):\n",
    "            if tmp > INTERPOLATION_NAN_GAP:\n",
    "                for j in range(x-tmp, x+1):\n",
    "                    nan_count[j] = SKIP_VALUE\n",
    "            tmp = 0\n",
    "    nan_count = nan_count.sort_index()\n",
    "    phrase_table[column] = phrase_table[column].interpolate() \\\n",
    "        .where(nan_count < SKIP_VALUE, np.nan)\n",
    "\n",
    "\n",
    "\n",
    "phrase_table = phrase_table.fillna(FILL_NAN_VALUE)\n",
    "df_concat = pd.concat([df_concat, phrase_table], axis = 0)\n",
    "df_concat'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing TODOs\n",
    "- goal: load first 30 parquet files\n",
    "- for each parquet file select only relevant columns (remove face date and pose date except for arms)\n",
    "- for each file remove NaNs (replace with -1 or interpolate if previous and following values are known)\n",
    "- add participant number and label phrase to parquet dfs\n",
    "- transform label phrase to array/string of numbers\n",
    "- combine dfs\n",
    "- export as one or two (hopefully not so) large pickle file\n",
    "\n",
    "Tips:\n",
    "- For each parquet file do preprocessing and save as pickle\n",
    "- later combine multiple pickle files to one file\n",
    "- to do preprocessing faster write preprocessing steps as function to only change name of file\n",
    "\n",
    "\n",
    "Next build a Transformer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "## Train_Landmarks\n",
    "Removing unnecessary columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_unwanted_columns(df):\n",
    "    # remove all pose columns except for the arm and wrist landmarks\n",
    "    to_remove_pose = []\n",
    "    for column in df.columns.tolist():\n",
    "        if 'pose' in column:\n",
    "            if '11' in column:\n",
    "                continue\n",
    "            elif '12' in column:\n",
    "                continue\n",
    "            elif '13' in column:\n",
    "                continue\n",
    "            elif '14' in column:\n",
    "                continue\n",
    "            elif '15' in column:\n",
    "                continue\n",
    "            elif '16' in column:\n",
    "                continue\n",
    "            else:\n",
    "                to_remove_pose.append(column)\n",
    "\n",
    "    df = df.drop(to_remove_pose, axis=1)\n",
    "\n",
    "\n",
    "    to_remove_face = []\n",
    "    for column in df.columns.tolist():\n",
    "        if 'face' in column:\n",
    "            to_remove_face.append(column)\n",
    "\n",
    "    df = df.drop(to_remove_face, axis=1)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Edit train_csv\n",
    "change Strings to number arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exchange_string_with_numbers_array(df, characters):\n",
    "    for i in range(0, len(df)):\n",
    "        num_array = [characters.get(char, char) for char in df.iloc[i]['phrase']]\n",
    "        df.at[i, 'phrase'] = num_array\n",
    "    columns = ['sequence_id', 'participant_id', 'phrase']\n",
    "    df_train_selected = df_train[columns]\n",
    "    return df_train_selected"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge datasets\n",
    "merge hand and arm data with participant id and phrase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_datasets(df, df_phrases):\n",
    "    df_merged = pd.merge(df, df_phrases, on='sequence_id', how = 'left')\n",
    "    return df_merged"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpolation of nan values\n",
    "interpolate for nan gaps shorter than 30 following directly, else replace nan values with impossible number value (= out of frame)\n",
    "This happens for each phrase dataset so we don't interpolate over multiple sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#trivial interpolation function\n",
    "INTERPOLATION_NAN_GAP = 30\n",
    "FILL_NAN_VALUE = -1\n",
    "SKIP_VALUE = 10000000\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def interpolate_dataset(df):\n",
    "    phrases = df.sequence_id.unique()\n",
    "    df_concat = pd.DataFrame()\n",
    "    for i in phrases:\n",
    "        phrase_table = df[df['sequence_id'] == i]\n",
    "        nan_count_left_hand = phrase_table['x_left_hand_0'].isna().sum()\n",
    "        nan_count_right_hand = phrase_table['x_right_hand_0'].isna().sum()\n",
    "        length_table = len(phrase_table)\n",
    "        if nan_count_left_hand > 0.75*length_table and nan_count_right_hand > 0.75*length_table:\n",
    "            continue\n",
    "        for column in phrase_table.columns.tolist():\n",
    "            if(column == 'sequence_id') or (column == 'frame') or (column == 'participant_id') or (column == 'phrase'):\n",
    "                continue\n",
    "            nan_count = phrase_table[column].isnull().astype(int).groupby(phrase_table[column].notnull().cumsum()).cumsum()\n",
    "            tmp = 0\n",
    "            for x in range(phrase_table.index[0],phrase_table.index[0] + len(nan_count)-1):\n",
    "                curr = nan_count[x]\n",
    "                next = nan_count[x+1]\n",
    "                if next > curr:\n",
    "                    tmp = next\n",
    "                    if (x == 541):\n",
    "                        if tmp > INTERPOLATION_NAN_GAP:\n",
    "                            for j in range(((x+2)-tmp), x+2):\n",
    "                                nan_count[j] = SKIP_VALUE\n",
    "                        tmp = 0\n",
    "                elif next == curr:\n",
    "                    tmp = 0\n",
    "                    continue\n",
    "                elif (next < curr):\n",
    "                    if tmp > INTERPOLATION_NAN_GAP:\n",
    "                        for j in range(x-tmp, x+1):\n",
    "                            nan_count[j] = SKIP_VALUE\n",
    "                    tmp = 0\n",
    "            nan_count = nan_count.sort_index()\n",
    "            phrase_table[column] = phrase_table[column].interpolate() \\\n",
    "                .where(nan_count < SKIP_VALUE, np.nan)\n",
    "\n",
    "        df_concat = pd.concat([df_concat, phrase_table], axis = 0)\n",
    "\n",
    "    df_concat = df_concat.fillna(FILL_NAN_VALUE)\n",
    "    return df_concat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create preprocessed Files\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./dataset complete/train.csv')\n",
    "\n",
    "with open (\"./dataset complete/character_to_prediction_index.json\", \"r\") as f:\n",
    "    characters = json.load(f)\n",
    "\n",
    "df_train = exchange_string_with_numbers_array(df_train, characters)\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "supp_train = pd.read_csv('./dataset complete/supplemental_metadata.csv')\n",
    "\n",
    "with open (\"./dataset complete/character_to_prediction_index.json\", \"r\") as f:\n",
    "    characters = json.load(f)\n",
    "\n",
    "supp_train = exchange_string_with_numbers_array(supp_train, characters)\n",
    "supp_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read in first parquet file\n",
    "train_landmarks = pq.read_table('./dataset complete/train_landmarks/105143404.parquet')\n",
    "train_landmarks = train_landmarks.to_pandas()\n",
    "train_landmarks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# leave only columns with hand and arm data\n",
    "train_landmarks = remove_unwanted_columns(train_landmarks)\n",
    "# merge the hand and arm data with the corresponding participant id and phrase\n",
    "df = merge_datasets(train_landmarks, df_train)\n",
    "# for short nan gaps interpolate the values, else fill Nans with a placeholder number\n",
    "df = interpolate_dataset(df)\n",
    "# check if all nans have been removed\n",
    "if len(df[df.isna().any(axis=1)]) != 0:\n",
    "    print(\"Still NANs!\")\n",
    "# print df\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save as Pickle file:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#file_path = './dataset complete/preprocessed_files/105143404.pkl'\n",
    "#df.to_pickle(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating preprocessed data files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_names = os.listdir('./dataset complete/train_landmarks/')\n",
    "for i in range(0, len(file_names)):\n",
    "    file_names[i] = file_names[i].replace('.parquet', '')\n",
    "\n",
    "file_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file in file_names:\n",
    "    print(file)\n",
    "\n",
    "    # read in first parquet file\n",
    "    train_landmarks = pq.read_table('./dataset complete/train_landmarks/' + file + '.parquet')\n",
    "    train_landmarks = train_landmarks.to_pandas()\n",
    "\n",
    "    # leave only columns with hand and arm data\n",
    "    train_landmarks = remove_unwanted_columns(train_landmarks)\n",
    "    # merge the hand and arm data with the corresponding participant id and phrase\n",
    "    df = merge_datasets(train_landmarks, df_train)\n",
    "    # for short nan gaps interpolate the values, else fill Nans with a placeholder number\n",
    "    df = interpolate_dataset(df)\n",
    "    # check if all nans have been removed\n",
    "    if len(df[df.isna().any(axis=1)]) != 0:\n",
    "        print(\"Still NANs!\")\n",
    "\n",
    "    file_path = './dataset complete/preprocessed_files/' + file + '.pkl'\n",
    "    df.to_pickle(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_names = os.listdir('./dataset complete/supplemental_landmarks/')\n",
    "for i in range(0, len(file_names)):\n",
    "    file_names[i] = file_names[i].replace('.parquet', '')\n",
    "\n",
    "file_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file in file_names:\n",
    "    print(file)\n",
    "\n",
    "    # read in first parquet file\n",
    "    train_landmarks = pq.read_table('./dataset complete/supplemental_landmarks/' + file + '.parquet')\n",
    "    train_landmarks = train_landmarks.to_pandas()\n",
    "\n",
    "    # leave only columns with hand and arm data\n",
    "    train_landmarks = remove_unwanted_columns(train_landmarks)\n",
    "    # merge the hand and arm data with the corresponding participant id and phrase\n",
    "    df = merge_datasets(train_landmarks, supp_train)\n",
    "    # for short nan gaps interpolate the values, else fill Nans with a placeholder number\n",
    "    df = interpolate_dataset(df)\n",
    "    # check if all nans have been removed\n",
    "    if len(df[df.isna().any(axis=1)]) != 0:\n",
    "        print(\"Still NANs!\")\n",
    "\n",
    "    file_path = './dataset complete/supp_preprocessed_files/' + file + '.pkl'\n",
    "    df.to_pickle(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in pickle files:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df_pkl = pd.read_pickle('./dataset complete/preprocessed_files/933868835.pkl')\n",
    "#print(len(df_pkl))\n",
    "#df_pkl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save as csv to have a look if dataset is preprocessed like we wanted\n",
    "#df_pkl.to_csv('./dataset complete/preprocessed_files/test.csv')\n",
    "#import os\n",
    "#path = './dataset complete/preprocessed_files/test.csv'\n",
    "#os.remove(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merge pickle files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df1 = pd.read_pickle('./dataset complete/preprocessed_files/5414471.pkl')\n",
    "#df2 = pd.read_pickle('./dataset complete/preprocessed_files/105143404.pkl')\n",
    "#df = pd.concat([df1, df2], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open (\"./dataset complete/character_to_prediction_index.json\", \"r\") as f:\n",
    "    characters = json.load(f)\n",
    "len(characters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformer import get_compiled_transformer\n",
    "\n",
    "# TODO: tune hyperparameters\n",
    "d_model = 128  # hidden layer(s) dimensionality\n",
    "num_layers = 5 # how many encoders and decoders to stack\n",
    "num_heads = 8 # how many attention heads should every mha have\n",
    "ff_dim = 256 # how many neurons shall feed-forward layers have\n",
    "dropout_rate = 0.1\n",
    "\n",
    "output_vocab_size = len(characters) + 2\n",
    "\n",
    "transformer = get_compiled_transformer(\n",
    "    d_model=d_model,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    dropout_rate=dropout_rate,\n",
    "    output_vocab_size=output_vocab_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_data = pd.read_pickle('./dataset complete/preprocessed_files/532011803.pkl')\n",
    "df_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Tune - the longer the worse is the performance\n",
    "MAX_PHRASE_LENGTH = 100\n",
    "MAX_SIGN_LENGTH = 800"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "phrases = df_data.sequence_id.unique()\n",
    "columns = ['sequence_id', 'frame', 'participant_id', 'phrase']\n",
    "targets = []\n",
    "inputs = []\n",
    "contexts = []\n",
    "for i in phrases:\n",
    "    phrase_table = df_data[df_data['sequence_id'] == i]\n",
    "    phrase = phrase_table.iloc[0].phrase\n",
    "    if len(phrase) > MAX_PHRASE_LENGTH:\n",
    "        print('Houston we have a problem!')\n",
    "    while len(phrase) < MAX_PHRASE_LENGTH:\n",
    "        phrase.append(59)\n",
    "    context = deepcopy(phrase)\n",
    "    context.insert(0, 60)\n",
    "    contexts.append(context[:-1])\n",
    "    targets.append(phrase)\n",
    "\n",
    "    phrase_table = phrase_table.drop(columns, axis=1)\n",
    "    phrase_table = phrase_table.to_numpy(copy=True)\n",
    "\n",
    "    if phrase_table.shape[0] > MAX_SIGN_LENGTH:\n",
    "        print('Houuuuuussssteeeeeen!')\n",
    "\n",
    "    phrase_table = np.pad(phrase_table, [(0, MAX_SIGN_LENGTH-phrase_table.shape[0]), (0,0)], 'constant', constant_values=(-1, -1))\n",
    "\n",
    "    inputs.append(phrase_table)\n",
    "\n",
    "inputs = np.stack(inputs)\n",
    "contexts = np.array(contexts)\n",
    "targets = np.array(targets)\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "print(contexts.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE=64\n",
    "\n",
    "inputs = [inputs, contexts]\n",
    "\n",
    "transformer.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T19:56:22.408925100Z",
     "start_time": "2023-06-20T19:56:20.602600900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T19:56:25.435836300Z",
     "start_time": "2023-06-20T19:56:25.421823600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T19:52:43.928092400Z",
     "start_time": "2023-06-20T19:52:43.920084100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Test Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "                                     path     file_id  sequence_id  \\\n0         train_landmarks/5414471.parquet     5414471   1816796431   \n1         train_landmarks/5414471.parquet     5414471   1816825349   \n2         train_landmarks/5414471.parquet     5414471   1816909464   \n3         train_landmarks/5414471.parquet     5414471   1816967051   \n4         train_landmarks/5414471.parquet     5414471   1817123330   \n...                                   ...         ...          ...   \n67203  train_landmarks/2118949241.parquet  2118949241    388192924   \n67204  train_landmarks/2118949241.parquet  2118949241    388225542   \n67205  train_landmarks/2118949241.parquet  2118949241    388232076   \n67206  train_landmarks/2118949241.parquet  2118949241    388235284   \n67207  train_landmarks/2118949241.parquet  2118949241    388332538   \n\n       participant_id                          phrase  \n0                 217                    3 creekhouse  \n1                 107                 scales/kuhaylah  \n2                   1             1383 william lanier  \n3                  63               988 franklin lane  \n4                  89       6920 northeast 661st road  \n...               ...                             ...  \n67203              88                    431-366-2913  \n67204             154                    994-392-3850  \n67205              95  https://www.tianjiagenomes.com  \n67206              36               90 kerwood circle  \n67207             176                      802 co 66b  \n\n[67208 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>file_id</th>\n      <th>sequence_id</th>\n      <th>participant_id</th>\n      <th>phrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816796431</td>\n      <td>217</td>\n      <td>3 creekhouse</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816825349</td>\n      <td>107</td>\n      <td>scales/kuhaylah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816909464</td>\n      <td>1</td>\n      <td>1383 william lanier</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816967051</td>\n      <td>63</td>\n      <td>988 franklin lane</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1817123330</td>\n      <td>89</td>\n      <td>6920 northeast 661st road</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>67203</th>\n      <td>train_landmarks/2118949241.parquet</td>\n      <td>2118949241</td>\n      <td>388192924</td>\n      <td>88</td>\n      <td>431-366-2913</td>\n    </tr>\n    <tr>\n      <th>67204</th>\n      <td>train_landmarks/2118949241.parquet</td>\n      <td>2118949241</td>\n      <td>388225542</td>\n      <td>154</td>\n      <td>994-392-3850</td>\n    </tr>\n    <tr>\n      <th>67205</th>\n      <td>train_landmarks/2118949241.parquet</td>\n      <td>2118949241</td>\n      <td>388232076</td>\n      <td>95</td>\n      <td>https://www.tianjiagenomes.com</td>\n    </tr>\n    <tr>\n      <th>67206</th>\n      <td>train_landmarks/2118949241.parquet</td>\n      <td>2118949241</td>\n      <td>388235284</td>\n      <td>36</td>\n      <td>90 kerwood circle</td>\n    </tr>\n    <tr>\n      <th>67207</th>\n      <td>train_landmarks/2118949241.parquet</td>\n      <td>2118949241</td>\n      <td>388332538</td>\n      <td>176</td>\n      <td>802 co 66b</td>\n    </tr>\n  </tbody>\n</table>\n<p>67208 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('./dataset complete/train.csv')\n",
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T20:33:42.559566400Z",
     "start_time": "2023-06-20T20:33:41.718760100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "94"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ids = df_test.participant_id.unique().tolist()\n",
    "len(p_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T20:34:06.923634200Z",
     "start_time": "2023-06-20T20:34:06.907115400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random\n",
    "TRAIN_SPLIT = 0.8\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "num_train_samples = int(TRAIN_SPLIT * len(p_ids))\n",
    "\n",
    "train_participants = random.sample(p_ids, num_train_samples)\n",
    "test_participants = [x for x in p_ids if x not in train_participants]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T20:34:09.273806100Z",
     "start_time": "2023-06-20T20:34:09.259794900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [01:09<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# TODO: Tune - the longer the worse is the performance\n",
    "MAX_PHRASE_LENGTH = 100\n",
    "MAX_SIGN_LENGTH = 900\n",
    "\n",
    "columns = ['sequence_id', 'frame', 'participant_id', 'phrase']\n",
    "targets_train = []\n",
    "inputs_train = []\n",
    "contexts_train = []\n",
    "targets_test = []\n",
    "inputs_test = []\n",
    "contexts_test = []\n",
    "\n",
    "files = os.listdir('./dataset complete/preprocessed_files')\n",
    "\n",
    "for file in tqdm(files):\n",
    "    df_data = pd.read_pickle('./dataset complete/preprocessed_files/' + file)\n",
    "    phrases = df_data.sequence_id.unique()\n",
    "\n",
    "    for i in phrases:\n",
    "        phrase_table = df_data[df_data['sequence_id'] == i]\n",
    "\n",
    "        is_test = False\n",
    "        if phrase_table.participant_id.unique()[0] in test_participants:\n",
    "            is_test = True\n",
    "\n",
    "        phrase = phrase_table.iloc[0].phrase\n",
    "        if len(phrase) > MAX_PHRASE_LENGTH:\n",
    "            print('Houston we have a problem!')\n",
    "        while len(phrase) < MAX_PHRASE_LENGTH:\n",
    "            phrase.append(59)\n",
    "        context = deepcopy(phrase)\n",
    "        context.insert(0, 60)\n",
    "\n",
    "        phrase_table = phrase_table.drop(columns, axis=1)\n",
    "        phrase_table = phrase_table.to_numpy(copy=True)\n",
    "\n",
    "        if phrase_table.shape[0] > MAX_SIGN_LENGTH:\n",
    "            print('Houuuuuussssteeeeeen!')\n",
    "\n",
    "        phrase_table = np.pad(phrase_table, [(0, MAX_SIGN_LENGTH-phrase_table.shape[0]), (0,0)], 'constant', constant_values=(-1, -1))\n",
    "\n",
    "        if not is_test:\n",
    "            inputs_train.append(phrase_table)\n",
    "            targets_train.append(phrase)\n",
    "            contexts_train.append(context[:-1])\n",
    "        else:\n",
    "            inputs_test.append(phrase_table)\n",
    "            targets_test.append(phrase)\n",
    "            contexts_test.append(context[:-1])\n",
    "\n",
    "inputs_train = np.stack(inputs_train)\n",
    "contexts_train = np.array(contexts_train)\n",
    "targets_train = np.array(targets_train)\n",
    "inputs_test = np.stack(inputs_test)\n",
    "contexts_test = np.array(contexts_test)\n",
    "targets_test = np.array(targets_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T20:36:49.371993400Z",
     "start_time": "2023-06-20T20:34:11.502867600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43110, 900, 144)\n",
      "(43110, 100)\n",
      "(43110, 100)\n"
     ]
    }
   ],
   "source": [
    "print(inputs_train.shape)\n",
    "print(targets_train.shape)\n",
    "print(contexts_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T20:37:01.599430900Z",
     "start_time": "2023-06-20T20:37:01.592424800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10143, 900, 144)\n",
      "(10143, 100)\n",
      "(10143, 100)\n"
     ]
    }
   ],
   "source": [
    "print(inputs_test.shape)\n",
    "print(targets_test.shape)\n",
    "print(contexts_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T20:37:03.111086Z",
     "start_time": "2023-06-20T20:37:03.093069400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./dataset complete/train_test_data/train_data.pkl', 'wb') as file:\n",
    "    train_data = [(inputs_train, contexts_train), targets_train]\n",
    "    pickle.dump(train_data, file)\n",
    "\n",
    "with open('./dataset complete/train_test_data/test_data.pkl', 'wb') as file:\n",
    "    test_data = [(inputs_test, contexts_test), targets_test]\n",
    "    pickle.dump(test_data, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-20T20:41:56.056772300Z",
     "start_time": "2023-06-20T20:39:41.084631Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
